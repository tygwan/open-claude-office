# Project Narratives

> 각 프로젝트의 문제 정의 → 해결 방법 → 실행 → 회고를 선택의 과정 중심으로 서술합니다.

---

## BIM Ontology — Semantic BIM Pipeline

BIM(건설 정보 모델링)은 설계과정에서 모두 다 다른 변수를 가집니다. 일반화할 수 있는 IFC 파일을 추출하면 계층 관계가 무시되며, Navisworks에서 CSV로 내보내면 프로퍼티 컨텍스트가 증발합니다. 엔지니어가 알고 싶은 것은 "이 공간에 인접한 방이 어디인가", "이 시스템에 속한 부재들은 무엇인가" 같은 관계형 질문으로부터 기존 도구로는 이런 질문에 답할 수 없었습니다. 단순히 데이터가 없어서가 아니라, 환경에 따라 내보내기 과정에서 관계 정보가 일관적이지 않으며 구조적으로 파괴되는 문제가 있씁니다..

이 문제를 해결하기 위해 시맨틱 웹 기술 기반의 Ontology를 선택했습니다. IFC 파일과 Navisworks CSV를 RDF 트리플로 변환하는 ETL 파이프라인을 설계하고, OWL/RDFS 기반 온톨로지로 원본 데이터의 계층 관계와 프로퍼티 컨텍스트를 복원하는 접근입니다. RDF를 선택한 이유는 관계형 DB는 스키마가 고정되어 BIM의 유연한 속성 체계를 담기 어렵고, 그래프 DB(Neo4j)는 표준화된 쿼리 언어와 추론 엔진이 부족합니다. RDF+SPARQL+SHACL 조합이 저장-질의-검증-추론을 하나의 표준 체계로 해결할 수 있는 선택지였습니다.

데이터를 저장하기 위해 선택한 것은 트리플스토어입니다. 초기에는 Python 생태계와의 호환성을 고려해 rdflib를 사용했지만, 2.8M 트리플을 로드하자 복잡한 SPARQL 쿼리에서 응답 시간이 분단위로 길어졌습니다. Oxigraph(Rust 기반)로 전환하면 34-64배 성능 향상이 가능했지만, 두 백엔드의 쿼리 결과에서 blank node 처리, ORDER BY 동작, 집계 함수 결과가 엔진마다 달랐습니다. 이를 해결하기 위해 Golden Query 프레임워크를 먼저 구축했습니다. 40개의 표준 SPARQL 쿼리를 5개 시맨틱 도메인(통계, 계층, 프로퍼티, 공간, 크로스도메인)에 걸쳐 정의하고, 기대 결과를 JSON 스냅샷으로 고정했습니다. 이 테스트가 있었기 때문에 Shadow/Canary 패턴으로 두 백엔드를 병렬 실행하면서 점진적으로 트래픽을 전환할 수 있었습니다. 결과적으로 p95 응답 517ms, 40개 Golden Query 100% 통과라는 기준을 달성하며 무중단 전환에 성공했습니다.

SPARQL이라는 진입 장벽도 풀어야 했습니다. 프로젝트 매니저나 안전 엔지니어가 "Zone A의 파이프 총 중량은?" 같은 질문을 하려면 SPARQL 문법을 알아야 하는데, 이는 비현실적인 사용조건입니다. NL2SPARQL 파이프라인을 구축하여 한국어/영어 자연어를 SPARQL로 변환하되, 스키마 리트리버가 온톨로지의 엔티티/프레디킷 매핑을 먼저 추출하고, 정적 검증기가 인젝션 공격(UNION, DROP, DELETE)을 차단하는 구조로 설계했습니다. 단순히 LLM에게 쿼리를 생성시키는 것이 아니라, 온톨로지 스키마로 범위를 제한하고 보안 검증을 거치는 신뢰할 수 있는 파이프라인을 만들었습니다.

돌이켜보면, Golden Query 프레임워크를 가장 먼저 구축한 것이 프로젝트 전체의 품질을 결정지었습니다. 40개의 테스트가 없었다면 Oxigraph 전환도, Delta 업데이트 엔진도, SHACL 검증 파이프라인도 안전하게 도입할 수 없었습니다. 측정할 수 없으면 개선할 수 없다는 원칙이 시맨틱 웹이라는 복잡한 기술 스택에서 특히 유효했습니다. 3주 61커밋이라는 빠른 속도에서도 품질을 유지할 수 있었던 것은, 속도를 내기 전에 Ultra-cc-init을 통해 검증 인프라를 먼저 깔았으며 Open-phanteon을 통해 품질을 검증하면서 진행했기 때문입니다.

---

## DXTnavis — BIM 4D Automation Plugin

Navisworks는 건설 업계의 표준 BIM 뷰어이지만, 내장된 도구는 한 번에 하나의 객체만 볼 수 있고, 4D 시뮬레이션(TimeLiner)을 설정하려면 수동으로 객체와 csv로된 일정을 하나하나 반복작업 해야합니다. 실제 BIM 모델은 적게는 수만에서 많개는 수백만개의 부재를 포함하는데, 이런 수동 워크플로우로는 현실적인 시간 안에 작업을 완료할 수 없습니다.

프로젝트의 첫 번째 관문은 Navisworks가 제공하는 .NET API의 근본적인 제약이였습니다. 이 API는 모델 데이터를 읽을 수는 있지만, 값을 쓸 수는 없습니다. 4D 자동화 파이프라인의 핵심은 스케줄 데이터를 모델 객체에 연결하는 것인데, 쓰기가 불가능하면 파이프라인 자체가 성립하지 않습니다. 대안은 세 가지로, 외부 DB에 매핑 테이블을 만드는 방법은 동기화 문제를 야기하고, IFC 파일을 직접 수정하는 방법은 쓰기가 안되기 때문에 불가능합니다. 남은 선택은 COM API였고, Navisworks의 .NET API와 COM API는 별개의 인터페이스인데, COM API의 `InwGUIPropertyNode2.SetUserDefined()`가 유일하게 커스텀 프로퍼티 쓰기를 허용하고 있습니다. 결과적으로 읽기는 .NET API로, 쓰기는 COM API로. 이 결정으로 아키텍처를 설정할 수 있었습니다다.

두 번째 관문은 성능이었습니다. 실제 설계된 BIM 모델로 테스트 했으며, 프로퍼티를 추출하면 약 445,000개의 정보가 등장합니다. WPF의 ObservableCollection에 이 데이터를 바인딩하면 UI가 완전히 멈추고, Select All 같은 기본 동작도 수 분이 걸렸습니다. 여기서 세 가지 접근을 검토했습니다. UI 가상화 만으로는 Select All 같은 전체 순회 연산의 근본적인 느림을 해결할 수 없었고, 페이지화는 BIM 워크플로우에서 사용성이 안좋았습니다. 결국 개별 프로퍼티 레코드를 객체 단위로 그룹화하여 데이터 구조 자체를 재설계했고, 445K 레코드를 약 5K 그룹으로 변환에 성공했습니다. 99%의 반복 횟수 감소. 이 리팩토링이 v1.0.0으로 출시되었고, 이를 통해 실질적인 플러그인 사용이 가능해졌습니다.

세 번째 관문은 BIM 포맷 간 호환성이었습니다. Navisworks는 Revit, CATIA, PDMS 와 같은 포맷을 로드할 수 있는데, CATIA나 PDMS에서 가져온 모델은 InstanceGuid가 `Guid.Empty`로 비어 있습니다. 이러면 부모-자식 계층이 무너지고, 외부 시스템(온톨로지, 스케줄링)과의 매핑도 어렵습니다. 랜덤 GUID를 생성하면 일관된 작업이 불가능하며, 인덱스 기반 ID는 모델 변경에 취약합니다. MD5 해시를 사용한 합성 ID를 도입했습니다. 입력이 같으면 항상 같은 ID가 생성되므로 세션 간 일관성이 보장되고, bim-ontology 프로젝트와의 연동에서도 안정적인 식별자 역할을 할 수 있었습니다.

44일간 16개 Phase를 거치며 v0.0.1에서 v1.4.0까지 진화한 과정에서 일관된 패턴이 있었습니다. 매 Phase마다 Navisworks라는 플랫폼의 제약에 부딪혔고, 그 제약 안에서 작동하는 해결책을 찾는 실용적 접근을 취했습니다. API가 읽기 전용이면 COM으로 우회하고, 데이터가 너무 많으면 구조를 재설계하고, ID가 없으면 일관된 값으로 생성하도록 만들었습니다. 제약을 정면으로 돌파하기보다 제한된 조건 안에서 최선을 찾는 방식이 BIM 도메인 플러그인 개발의 핵심이었습니다.

---

## Resumely — AI Cover Letter SaaS

국내 취업 시장에서 자기소개서는 지원자에게 가장 고통스러운 과정중 하나입니다. 기업마다 다른 문항, 글자수 제한, 직무별 키워드 요구사항이 있고, 매 공고마다 자신의 경험을 새로운 구조로 재구성해야 합니다. ChatGPT나 Claude와 같은 LLM 기반 AI 작성 도구들은 두 가지 핵심 문제를 안고 있었습니다. 첫째, "열정을 가지고", "기여하겠습니다" 같은 AI 특유의 클리셰가 반복되어 AI로 성의없이 작성된 게 드러납니다. 둘째, 지원자의 실제 경험을 채용공고의 요구 역량과 정밀하게 연결하기 어렵습니다.

이 문제에 대한 접근은 판단 기준 중심의 구조화 입니다. 인사담당자 관점의 7단계 자기소개서 프레임워크를 프롬프트 시스템을 내장해서, LLM이 단순히 글을 쓰는 것이 아니라 왜 그렇게 했는지에 대한 사고 흐름으로 작성하도록 유도했습니다. AI가 쓴 티를 없애는 것은 프롬프트 하나로 해결되지 않습니다. 3중 품질 보증 체계 즉, 규칙을 설계했습니다. Few-shot examples로 좋은/나쁜 예시를 프롬프트에 포함하고, 금지 문구 16개 탐지하는 validate-output 검증 엔진, K-STAR-K 패턴 준수 검증, 4종 안티패턴 감지를 수행하며, 임계값 초과 시 수정 피드백 과정을 거치며 재생성합니다. 프롬프트 예방만으로 충분하면 검증 불필요, 불충분하면 검증 레이어 추가라는 분기점을 추가하였고, 이러한 사전 예방만으로는 100% 방지가 불가능했기에 사용자의 최종 개입을 유도했습니다다.

SaaS로서 체감 품질을 결정짓는 또 하나의 기술적 결정은 SSE 실시간 스트리밍이었습니다. 자기소개서 생성에 약 60초가 소요되는데, 이 동안 사용자는 LLM이 작성중인지 확인할 방법이 없습니다. Vercel AI SDK의 기본 훅은 JSON 스트림만 지원하고, 생성 후 품질 검증이나 자동 재시도 같은 후처리 이벤트를 클라이언트에 전달할 채널이 없었습니다. WebSocket은 Vercel 서버리스에서 연결 유지가 어렵고, Polling은 실시간성이 부족합니다. 커스텀 SSE 프로토콜을 설계하여 6종 이벤트 타입(started/partial/validating/retrying/complete/error)을 정의했습니다. 이로써 체감 대기 시간을 약 30초에서 2-3초로 단축하고, 생성-검증-재시도 전체 생명주기를 사용자에게 실시간으로 보여줄 수 있게 되었습니다.

LLM API 모델에서도 선택의 과정이 있었습니다. Anthropic, OpenAI, Google의 8개의 API 모델을 3-tier로 빠른건 1크레딧, 균형잡인건 3크레딧, 추론을 필요로 한다면 10크레딧으로 분류했습니다. 가격 결정의 핵심 기준은 API 비용 역산입니다. 각 모델의 토큰 단가를 기준으로 원가 대비 마진을 설정하여 크레딧 비용을 산출했습니다. 구독제 대신 종량제를 택한 이유는 MVP 단계에서 가격 실험이 용이하기 때문이고, 출력 형태가 정해진 분석단계는 저가 모델을 사용하고 추론이 필요한 생성과정만 유료화하여 진입 장벽을 최소화했다. 무료 5크레딧 체험 후 유료 전환을 유도했습니다.

23일 만에 인증, 결제, Multi-model AI, i18n, 실시간 스트리밍, 품질 검증이 모두 포함된 프로덕션 SaaS를 출시할 수 있었던 것은 세 가지 판단 덕분이였습니다. 첫째, 핵심 로직을 단일 Google의 Gems를 통해 프로토타입으로 UX UI를 먼저 검증한 뒤 풀스택으로 전환했습니다. 로직 검증 없이 스택 선택을 하면 전체 시간이 낭비될 수 있습니다. 둘째, 커스텀 백엔드가 필요 없으면 Baas 즉 Supabase로 위임한다는 원칙으로 Auth, DB, Storage를 전부 Supabase에 맡겼습니다. 셋째, 23일이라는 시간 안에서 속도와 품질의 균형을 가능하게 했던 것은, 속도를 내기 전에 Ultra-cc-init을 통해 개발 계획과 사용 스택을 선정하여 개발 내용을 상세히 문서화 하여 일관성을 유지했습니다. 또한, Open-phanteon을 통해 품질을 검증하면서 진행했습니다.

---

## open-pantheon — Multi-CLI AI Orchestration Framework

Claude Code, Codex CLI, Gemini CLI와 같은 AI 코딩 에이전트는 각각 강력하지만 독립적인 프레임워크 입니다. 프로젝트를 시작하면 초기화, 문서 생성, Phase 관리, 품질 검증을 사용자가 수동으로 목표를 설정하고 문서작성하며, 완성된 프로젝트를 포트폴리오로 전환하려면 전혀 다른 워크플로우가 필요합니다. 더 근본적인 문제는 단일 AI의 blind spot입니다. 하나의 LLM에 분석, 디자인, 코드 생성, 검증을 모두 맡기면 각 작업에 최적화된 모델을 활용할 수 없고, 기능을 확장할수록 컨텍스트 윈도우가 포화되어 오히려 성능이 저하되는 역설이 발생합니다. AI Native 환경에서 Agent를 이용할 때에는 속도도 속도지만 정확도도 매우 중요합니다.

이 프로젝트는 47일간 세 번의 진화를 거쳤습니다. 첫 번째 형태인 cc-initializer는 Claude Code 세션을 위한 프로젝트 초기화 도구였습니다. CLAUDE.md 자동 생성, 기본적인 Phase 관리가 전부였지만, 38개 커밋 동안 폭발적으로 성장하면서 v4.5에 이르러 20개 이상의 에이전트와 스킬을 갖춘 프레임워크가 되었습니다. 그러자 20개 넘는 에이전트를 모두 로드하면 서 컨텍스트 윈도우가 포화되는 토큰 병목현상이 발생했습니다.

두 번째 형태인 ultra-cc-init은 이 토큰 문제를 해결했습니다. Agent MANIFEST를 도입하여 키워드 매칭 기반 lazy-loading을 구현했고, 4-tier 토큰 예산 체계(quick 2K, standard 10K, deep 30K, full 50K)로 작업 유형별 컨텍스트 크기를 제어했습니다. 이 때에 Codex CLI를 dual-AI loop로 통합하여 Claude가 구현하고 Codex가 검증하는 Multi-AI 협업 패턴을 확립한 것이 결정적이었습니다. 하나의 AI가 모든 것을 하는 대신, 역할을 분리하면 각각의 강점을 극대화할 수 있다는 통찰입니다다.

세 번째이자 최종 형태인 open-pantheon은 별도로 개발되던 포트폴리오 생성 파이프라인인 foliocraft 프로젝트와 ultra-cc-init 그리고 오픈소스인 claw-empire의 체계를 합병한 것입니다. 합병 과정에서 가장 어려운 문제는 세가지 시스템의 이벤트 체계를 연결하는 것이었습니다. Craft 파이프라인의 상태 전이와 Dev Lifecycle의 Phase 관리/Sprint/Quality Gate가 원래 별개의 이벤트 흐름을 가지고 있었습니다. state-transition.sh와 craft-progress.sh라는 두 개의 bridge hook을 신규 도입하여, `.state.yaml`로 변경을 감지하면 Quality Gate 트리거, Feedback Loop 연동, PROGRESS.md 동기화를 자동으로 실행하는 구조를 만들었습니다.

Multi-CLI 아키텍처에서 핵심은 쓰기 권한의 분배였습니다. Codex CLI에 읽기만을 강제하여 분석과 검증만 수행하게 하고, 모든 파일 수정은 Claude가 담당했습니다. 각 CLI에 쓰기 권한을 모두 주면 산출물 일관성을 제어하기 어렵고 충돌 위험이 있기 때문입니다. Gemini CLI에는 auto-approve를 적용하여 비대화형으로 실행합니다. 이 결정 덕분에 분석은 Codex가, 디자인은 Gemini의 시각적 창의력, 종합 판단과 코드 생성은 Claude의 추론력이라는 역할 분담이 깔끔하게 작동합니다.

또 하나의 근본적 결정은 코드 없는 프레임워크를 만든 것입니다. 26개 에이전트, 29개 스킬, 13개 커맨드를 전부 Markdown 파일로 정의했습니다. TypeScript나 Python으로 에이전트를 구현할 수도 있었지만, Claude Code의 Agent Native 시스템과 직접 호환되는 Markdown이 투명성과 수정 용이성 면에서 압도적이었습니다. 에이전트의 행동을 바꾸려면 코드를 디버깅하는 것이 아니라 Markdown을 편집하면 되기 때문입니다. 39줄짜리 MANIFEST.md만 먼저 로드하고 매칭된 에이전트만 lazy-load하는 구조는 토큰 최적화의 핵심이기도 합니다.

47일, 79커밋, 21K에서 29.5K 코드 변화로의 성장을 돌아보면, 이 프로젝트의 핵심은 기술적 복잡도의 관리였습니다. 에이전트와 스킬이 늘어날수록 시스템이 복잡해지는 것은 필연이지만, 그 복잡도를 MANIFEST 라우팅, 토큰 예산, 상태머신 guard, Quality Gate로 제어 가능한 수준으로 유지했습니다. Claude의 초기화 토큰이 38K에서 1.1K로 97% 절감된 것은 단순한 최적화가 아니라, 컨텍스트를 사전에 모두 로드하는 것이 아니라 필요할 때 선택적으로 활성화한다는 아키텍처적 전환의 결과입니다다.
